---
title: "Problem_Set_3"
output: html_document
date: '2023-09-26'
---
1)
```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(iml)
library(tidyverse)
library(GGally)
library(reshape2)
library(iml)
library(dplyr)


# Load the dataset
corolla_data = read_csv("/Users/mercedesduncanson/Downloads/Repos/Problem_Set_3/ToyotaCorolla.csv")

# Explore the data
str(corolla_data)
summary(corolla_data)
glimpse(cars)

```
2)
```{r}
# Check for missing values
missing_values <- colSums(is.na(corolla_data))
missing_values
print(missing_values)

# Check if there are missing values in a variable, there are no missing values though so I left it as Price
if (sum(is.na(corolla_data$Price)) > 0) {
  # Impute missing values with the mean
  corolla_data$Price[is.na(corolla_data$Price)] <- mean(corolla_data$Price, na.rm = TRUE)
} else {
  # When there are no missing values
  cat("No missing values in Price column.\n")
}
```
3)
```{r}
# Check Price variable distribution
hist(corolla_data$Price)

# Consider transformations if necessary (e.g., log transformation)
corolla_data$LogPrice <- log(corolla_data$Price)
hist(corolla_data$LogPrice)

```
4)
```{r} 
#| message: false
# create correlation matrix to identify potential relationships, also I know I did it 2 times, it was to test another method vs the code we used in class.
corolla_data = corolla_data %>%
  select(-Id, -Model, -Mfg_Month, -Cylinders)

cars_fct = corolla_data %>%
  select(-Price, -Age_08_04, -KM, -HP, -CC, -Quarterly_Tax, -Weight) %>%
  mutate_all(.funs = factor)

cars_num = corolla_data %>%
  select(Price, Age_08_04, KM, HP, CC, Quarterly_Tax, Weight)

cars2 = bind_cols(cars_num, cars_fct)

cars2 %>%
  keep(is.numeric) %>%
  ggpairs()

```
We can see that there is a negative linear relationship with price and age, also price and KM. There is a positive linear relationship between age and KM.

5)
```{r}
# Calculate correlation matrix
correlation_matrix <- cor(corolla_data[, c("Age_08_04", "KM", "HP", "CC", "Quarterly_Tax", "Weight", "Mfg_Year")])

# Melt the correlation matrix and round the values for better readability
melted_correlation <- melt(correlation_matrix)
melted_correlation$value <- round(melted_correlation$value, 2)

# Create a heatmap with correlation values
ggplot(data = melted_correlation, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = value), vjust = 1) +  # Add correlation values as text labels
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

```
When looking at the heat map we can see that the highest positive correlations are between, weight and quarterly_tax, age and KM, and Mfg_Year, and weight,  none of their numbers are concerning though since they're not above 0.7 so they should all still be included. The highest negative correlation that we can see is between Mfg_Year and Age_08_04, this is because these are ecnetially the same thing, manufacturing year and the cars "age" are most often only months apart aand are definetly seen as the same thing. I would suggest removing Mfg_Year and using Age_08_04, since this would be the date the cars are available for sale. Mfg_Year does get removed in the next few steps.

6)
```{r}
# Set a random seed for reproducibility
set.seed(123)

# Split the data into training (70%) and testing (30%) sets
sample_size <- floor(0.7 * nrow(corolla_data))
train_indices <- sample(seq_len(nrow(corolla_data)), size = sample_size)

# Create the training set
train_data <- corolla_data[train_indices, ]

# Create the testing set (the remaining data)
test_data <- corolla_data[-train_indices, ]
```

7)
```{r}
# Train a regression tree model
model <- rpart(Price ~ ., data = train_data)

# Cross-validation to find the best cost complexity parameter (cp)
cp_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
cv_model <- train(Price ~ ., data = train_data, method = "rpart", tuneGrid = cp_grid)
best_cp <- cv_model$bestTune$cp

# Prune the tree with the best cp
pruned_model <- prune(model, cp = best_cp)

# Visualize the pruned tree
rpart.plot(pruned_model, extra = 101)


```
This assignment primarily emphasizes post-pruning through cross-validation to optimize the regression tree model's complexity and ensure that it generalizes well to new data. Pre-pruning techniques, such as setting constraints during tree construction, are not done or needed with this model.

8)
```{r}
# Train the regression tree model again
model_selected <- rpart(Price ~ ., data = train_data)

# Calculate variable importance using the built-in function
var_importance <- varImp(model_selected)

# Print the variable importance scores
print(var_importance)

```

9)
```{r}
# Remove less important features by only selecting the most important ones
selected_features <- c("LogPrice", "Weight", "Age_08_04")
train_data_selected <- train_data[, c(selected_features, "Price")]
test_data_selected <- test_data[, c(selected_features, "Price")]

# Retrain the pruned model
model_selected <- rpart(Price ~ ., data = train_data_selected)

```
Using the information we've gathered from the steps above, we can tell that Price, Weight, and Age, are the most important features that we can use to measure the cars. By selecting just these ones we simplifying our data while keeping it accurate and relavent to what our needs are/what's important to buyers.

10)
```{r}
# Make predictions on the test data
predictions <- predict(model_selected, newdata = test_data_selected)

# Calculate RMSE (Root Mean Square Error)
rmse <- sqrt(mean((test_data_selected$Price - predictions)^2))
rmse

# Interpretation of prediction error
# A lower RMSE indicates a more accurate pricing model. You can compare it to the cross-validation error to assess how well the model works.

# Access the RMSE for different values of cp from the cv_model (gives a vector of RMSE values for each tested value of cp)
cv_model$results$RMSE
```
the rmse (863.13r), is similar the lowest two cp values, 737.45r and 912.43r. Aside from those 2, the rmse get lower and lower the further down the decision tree we test, this can mean that there are some further measures we could take to better train our model, but it still can perform decently on testing data.

